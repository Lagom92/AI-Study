{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitd6c49ce9a0fb4da9a60aa5dde01a050b",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.5. 단순 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 선형 회귀의 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 회귀의 목표는 연속형 반응 변수(출력결과)의 값을 예측하는 것\n",
    "\n",
    "- 단순 선형 회귀란 설명 변수인 단일 특징과 단일 반응 변수 간에 선형 관계가 있다고 가정하고 초평면이라고 하는 선형 평면을 이용해 모델링한 것\n",
    "\n",
    "- 단순 회귀는 설명 변수의 차원과 반응 변수의 차원, 모두 2개의 차원을 가지며 초평면은 한차원 낮은 1차원이 됨(1차원의 초평면은 선)\n",
    "\n",
    "- y = a + bx\n",
    "    - y: 반응 변수의 예측값\n",
    "    - 절편항 a와 계수 b: 알고리즘을 통해 학습하게 되는 모델의 파라미터\n",
    "    - x: 설명 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최소 제곱법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OLS(Ordinary Least Squares) 또는 LLS(Linear Least Squares)로 부름\n",
    "\n",
    "- 단순 선형 회귀에서 모델을 최적화하는 파라미터 값을 훈련 데이터로부터 학습하는 방법\n",
    "\n",
    "- 비용 함수: 손실 함수, 모델의 오차를 정의하고 측정하기 위해 사용\n",
    "\n",
    "- 잔차: residual, 훈련 오차로 훈련 데이터의 관측값과 모델에 의한 예측값의 차이\n",
    "\n",
    "- 예측 오차: prediction error, 테스트 오차로 테스트 데이터의 관측값과 모델에 의한 예측값의 차이\n",
    "\n",
    "- 잔차의 합을 최소화 ==> 값을 예측할 수 있는 추정기(estimator)를 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단순 선형 회귀 모델의 최적화 파라미터값을 정하기 위해 최소 제곱법을 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비용함수를 사용한 모델의 적합화 척도_RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 반응 변수의 예측값이 훈련 데이터의 관측값과 가까워지면 모델이 적합화되었다고 할 수 있음\n",
    "\n",
    "- 모델의 적합화의 척도: 잔차 제곱 합(RSS, Residual Sum of Squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 관측갑소가 예측값이 가까워진다는 것은 차이가 0에 가깝다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 선형 회귀를 위한 OLS의 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단순 선형 회귀식: y = a + bx\n",
    "\n",
    "- 비용 함수를 최소화 하는 a값과 b값을 찾는 것이 목표\n",
    "\n",
    "- b를 계산하기 위해 x의 분산과 x와 y의 공분산을 계산\n",
    "\n",
    "- var(x), cov(x, y)\n",
    "\n",
    "- b의 값 계산: b = cov(x, y) / var(x)\n",
    "\n",
    "- a의 값 계산: a = y_bar - b*x_bar\n",
    "    - x_bar: x의 평균\n",
    "    - y_bar: y의 평균\n",
    "\n",
    "- x, y 지점은 반드시 지나가야 하는 센트로이드 좌표이므로 평균값을 넣음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 선형 회귀 모델의 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 회귀 모델의 기본 예측 능력 평가 지표: 결정계수 R^2\n",
    "\n",
    "- 데이터가 회귀선에 얼마나 가깝게 분포하는지를 측정\n",
    "\n",
    "- 모델에 의해 설명된 반응 변수 분산의 비율을 기술 ==>  0 <= R^2 <= 1\n",
    "\n",
    "- R^2은 이상치에 민감하고, 모델에 새로운 특징이 추가되면 값이 증가하는 문제를 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Reference\n",
    "\n",
    "https://kbig.kr/portal//kbig/datacube/onl_edu_class/python?bltnNo=11583395976711"
   ]
  }
 ]
}